<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>

<link rel="icon" href="./Files/images/seal_icon.png">

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="“width=800”">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">

    @font-face {font-family: "FuturaBookC";
        src: url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.eot"); /* IE9*/
        src: url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.eot?#iefix") format("embedded-opentype"), /* IE6-IE8 */
        url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.woff2") format("woff2"), /* chrome firefox */
        url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.woff") format("woff"), /* chrome firefox */
        url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.ttf") format("truetype"), /* chrome firefox opera Safari, Android, iOS 4.2+*/
        url("https://db.onlinewebfonts.com/t/e05b78cd627ded97c38881306e3601fe.svg#FuturaBookC") format("svg"); /* iOS 4.1- */
    }

    .names {
      font-size: 14.3px;
    }
    .abstract {
      font-size: 16.3px;
    }
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: "FuturaBookC";
    font-size: 20px
    }
    strong {
    font-family: "FuturaBookC";
    font-size: 20px;
    }
    heading {
    font-family: "FuturaBookC";
    font-size: 27px;
    }
    headingText {
    font-family: "FuturaBookC";
    font-size: 20px;
    }
    papertitle {
    font-family: "FuturaBookC";
    font-size: 20px;
    font-weight: 700
    }
    venuetitle {
    font-family: "FuturaBookC";
    font-size: 16px;
    font-weight: 700
    }
    name {
    font-family: "FuturaBookC";
    font-size: 38px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }

    hr.new1 {
      border: 1px solid black;
      border-radius: 5px;
    }
  </style>
  <title>Hossein Askari</title>

  </head>


  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
        <p align="center">
          <name>MohammadHossein AskariHemmat</name>
        </p>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
        <p align="justify">
          I am a Phd student at <a href="http://www.polymtl.ca/en"> Ecole Polytechnique de Montreal </a>. I am working under supervision of <a href="https://www.polymtl.ca/expertises/david-jean-pierre"> Jean-Pierre David </a>  and  <a href="https://www.polymtl.ca/expertises/david-jean-pierre"> Yvon Savaria </a> I have receieved my Masters in Electrical and Computer Engineering and my Bachelors in Electrical Engineering from <a href="https://www.concordia.ca/">Concordia University</a> and <a href="https://uk.ac.ir/en/home"> Shahid Bahonar University</a> respectively. Prior to my PhD, I worked as an ASIC Verification Engineer at Microsemi and as a Software Engineer at Tru Simulation + Training.
        </p>

        <p align="center">
          <a href="mailto:m.h.askari.hemmat@gmail.com">Email</a> &nbsp/&nbsp
          <a href="https://scholar.google.ca/citations?user=ZGDfPM4AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
          <a href="https://github.com/hossein1387">Github</a> &nbsp/&nbsp
          <a href="http://ca.linkedin.com/in/askarihemmat">Linkedin</a> &nbsp/&nbsp
          <a href="Files/docs/HosseinAskari-CV.pdf">CV</a> 
        </p>
        </td>
        <td width="33%">
        <img src="./Files/images/Hossein.jpg"  alt="prl" width="250" height="246">
        </td>
      </tr>
      </tbody></table>

      <heading>Research</heading> <br> <br>
        <headingText>
          My main area of research is making Deep Neural Networks more computationally efficient. I approach this problem by proposing new algorithms and custom hardwares. 
        </headingText>

      <br><br><br><br>
      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody><tr>
          <td width="25%"><img src="./Files/images/quark.png" alt="prl" width="250" height="230"></td>
          <td width="75%" valign="center">
            <a href="https://github.com/PolyMTL-Gr2m/ara/tree/quark">
              <papertitle>Quark: An Integer RISC-V Vector Processor for Sub-byte Quantized DNN Inference</papertitle>
            </a>
            <br>
            <venuetitle> ISCAS 2023</venuetitle>
            <br>
              <a href="https://github.com/PolyMTL-Gr2m/ara/tree/quark"><img width="20px" src="./Files/logos/Github.png"></a> 
            </p>
          <p align="justify" class="abstract">
           Implemented in GF 22 FDX, Quark is an integer RISC-V vector processor that is designed to perform
           sub-byte computation for Quantized Neural Network inference. 
          </p>
          </td>
        </tr>
        </tbody></table>
        <!-- END OF POST -->

      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody><tr>
          <td width="25%"><img src="./Files/images/BARVINN_LOGO.png" alt="prl" width="250" height="100"></td>
          <td width="75%" valign="center">
            <a href="https://barvinn.readthedocs.io/en/latest/">
              <papertitle>BARVINN: Arbitrary Precision DNN Accelerator Controlled by a RISC-V CPU</papertitle>
            </a>
            <br>
            <venuetitle> ASP-DAC 2023</venuetitle>
            <br>
              <a href="https://github.com/hossein1387/BARVINN"><img width="20px" src="./Files/logos/Github.png"></a> 
              /
              <a href="https://arxiv.org/pdf/2301.00290.pdf"><img width="20px" src="./Files/logos/arxiv.jpg"></a> 
            </p>
          <p align="justify" class="abstract">
           In need of an arbitrary precision DNN accelerator? Checkout <a href="https://barvinn.readthedocs.io/en/latest/">BARVINN!</a> an
           open source FPGA DNN accelerator.
          </p>
          </td>
        </tr>
        </tbody></table>
        <!-- END OF POST -->

      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody><tr>
          <td width="25%"><img src="./Files/images/DeepliteRT.jpeg" alt="prl" width="250" height="250"></td>
          <td width="75%" valign="center">
            <a href="https://arxiv.org/pdf/2207.08820.pdf">
              <papertitle>Accelerating Deep Learning Model Inference on Arm CPUs with Ultra-Low Bit Quantization and Runtime</papertitle>
            </a>
            <br>
            <venuetitle> Embedded World 2022 </venuetitle>
            <br>
              <a href="https://arxiv.org/pdf/2207.08820.pdf"><img width="20px" src="./Files/logos/arxiv.jpg"></a> 
            </p>
          <p align="justify" class="abstract">
           Performing Ultra-Low bit inference computation on ARM CPUs. This paper provides 2 bit model running on commodity hardware.
           Mixed precision approach was used to minimize the accuracy loss. Novel method is proposed to run the 2 bit models on Arm Cortex A devices.
           Benchmark on classification and Object detection models are presented.
          </p>
          </td>
        </tr>
        </tbody></table>
        <!-- END OF POST -->

        <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody><tr>
          <td width="25%"><img src="./Files/images/QReg.gif" alt="prl" width="250" height="250"></td>
          <td width="75%" valign="center">
            <a href="https://arxiv.org/pdf/2206.12372.pdf">
              <papertitle>QReg: On Regularization Effects of Quantization</papertitle>
            </a>
            <br>
            <venuetitle> ICML 2022 (HAET) </venuetitle>
            <br>
              <a href="https://arxiv.org/pdf/2206.12372.pdf"><img width="20px" src="./Files/logos/arxiv.jpg"></a> 
              / 
              <a href="./Files/docs/Presentation_HAET2022.pdf"><img width="20px" src="./Files/logos/presentation_ico.png"></a>
              / 
              <a href="./Files/docs/Qreg_Poster.pdf"><img width="20px" src="./Files/logos/poster_ico.png"></a>
            </p>
          <p align="justify" class="abstract">
           In this paper, we explored how quantization effects training. More specifically, we study the regularization effect of quantization. We show that regardless of dataset, model, quantization level and technique, 8-bit quantization is a reliable source of regularization.
          </p>
          <p></p>
          </td>
        </tr>
        </tbody></table>
        <!-- END OF POST -->
  
      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Files/images/pito_mvu.png" alt="prl" width="250" height="150"></td>
        <td width="75%" valign="top">
          <a href="https://github.com/hossein1387/pito_riscv">
            <papertitle>RISC-V Barrel Processor for Deep Neural Network Acceleration</papertitle>
          </a>
          <br>
          <venuetitle> ISCAS 2021 </venuetitle>
          <br>
            <a href="https://ieeexplore.ieee.org/document/9401617"><img width="20px" src="./Files/logos/IEEE.png"></a> 
                  /
            <a href="https://drive.google.com/file/d/1vXfMpaZd6JOaJ-p3J5MhZSftGAhVnmsF/view?usp=sharing"><img width="20px" src="./Files/logos/presentation_ico.png"></a>
        </p>
        <p align="justify" class="abstract">
          Based on the architecture proposed in our FCCM 2020 paper, we built a RISC-V core that is connected to a neural network accelerator capable of performing Matrix Vector product. We used this system to compute a GEMV operation with an input matrix size of 8 by 128 and a weight matrix size of 128 by 128 with two-bit precision in only 16 clock cycles.
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->


      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Files/images/deepliteneutrino.jpeg" alt="prl" width="250" height="250"></td>

        <td width="75%" valign="top">
          <a href="https://docs.deeplite.ai/neutrino/index.html">
            <papertitle>Deeplite NeutrinoTM: A BlackBox Framework for Constrained Deep Learning Model Optimization</papertitle>
          </a> 
          <br>
          <a style="color:red"> Won IAAI Deployed Application Award!</a>
          <br>
          <venuetitle> AAAI 2021 (IAAI Technical Track) </venuetitle>
          <br>
          <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17780"><img width="20px" src="./Files/logos/report.jpg"></a> 
                  /
          <a href="https://github.com/Deeplite/neutrino"><img width="20px" src="./Files/logos/Github.png"></a>
        <p align="justify" class="abstract">
          In this work, we introduce a black-box framework, Deeplite Neutrino^{TM} for production-ready optimization of deep learning models. The framework provides an easy mechanism for the end-users to provide constraints such as a tolerable drop in accuracy or target size of the optimized models, to guide the whole optimization process
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->


      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Files/images/pito_pipeline.png" alt="prl" width="250" height="160"></td>
        <td width="75%" valign="top">
          <a href="./Files/docs/FCCM2020.pdf">
            <papertitle>RISC-V Barrel Processor for Accelerator Control</papertitle>
          </a>
          <br>
          <venuetitle> FCCM 2020 </venuetitle>
          <br>
        <a href="./Files/docs/FCCM2020.pdf"><img width="20px" src="./Files/logos/report.jpg"></a> 
          /
        <a href="https://drive.google.com/open?id=11VnbQI0LvuQH-P55d60Md0v0fSg0HN1p"><img width="20px" src="./Files/logos/presentation_ico.png"></a>

        <p align="justify" class="abstract">
          In this paper we designed a Barrel RISC-V processor. We used 8 harts (hardware threads) to control 8 Matrix Vector Units for a Deep Neural Network application.  We have implemented our design on a Xilinx Ultrascale FPGA. Our 8-hart barrel processor runs at 350 MHz with CPI of 1 and consumes 0.287W.
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->


      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%">
          <img src="./Files/images/MIDL.png" alt="prl" width="250" height="250">
        </td>

        <td width="75%" valign="top">
            <a href="https://github.com/hossein1387 U-Net-Fixed-Point-Quantization-for-Medical-Image-Segmentation">
              <papertitle>U-Net Fixed Point Quantization For Medical Image Segmentation
            </a>
          <br>
          <venuetitle> MICCAI 2019 </venuetitle>
          <br>
            <a href="https://arxiv.org/abs/1908.01073"><img width="20px" src="./Files/logos/arxiv.jpg"></a> 
            /
            <a href="./Files/docs/MICCAI2019_Presentation.pdf"><img width="20px" src="./Files/logos/presentation_ico.png"></a>
            /
            <a href="https://github.com/hossein1387/U-Net-Fixed-Point-Quantization-for-Medical-Image-Segmentation"><img width="20px" src="./Files/logos/Github.png"></a>
            <p align="justify" style="width:500px;">
              In this work, we present a fixed point quantization method for the U-Net architecture, a popular model in medical image segmentation. We then applied our quantization algorithm to three different datasets and comapred our results with the existing work. Our quantization method is more flexible (different quantization level is possible) compared to existing work. 
            </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->


      <!-- POST -->
<!--       <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Files/images/iscas2016.png" alt="prl" width="250" height="250"></td>
        <td width="75%" valign="top">
          <a href="https://github.com/hossein1387/BART-System-Verification">
            <papertitle>Towards code generation for ARM Cortex-M MCUs from SysML activity diagrams</papertitle>
          </a>
          <br>
          <venuetitle> ISCAS 2016 </venuetitle>
          <br>
            <a href="https://ieeexplore.ieee.org/document/7527404"><img width="20px" src="./Files/logos/IEEE.png"></a> 
            /
            <a href="https://github.com/hossein1387/BART-System-Verification"><img width="20px" src="./Files/logos/Github.png"></a>

          <p align="justify" class="abstract">
            In this work, we redefined New Activity Calculus (NuAC) terms to support code generation for ARM Cortex-M processors and we present an automated SysML activity diagram to RTX (Keil Real-Time Operating System) code generator that uses mapping rules expressed in NuAC.
          </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
 -->      <!-- END OF POST -->

      <!-- POST --><!-- 
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Files/images/BART_used_Track.png" alt="prl" width="250" height="200"></td>
        <td width="75%" valign="top">
          <a href="https://ieeexplore.ieee.org/document/7438006">
            <papertitle>Formal modeling, verification and implementation of a train control system</papertitle>
          </a>
          <br>
          <venuetitle> ICM 2015 </venuetitle>
          <br>
          <a href="https://ieeexplore.ieee.org/document/7438006"><img width="20px" src="./Files/logos/IEEE.png"></a> 
          /
          <a href="https://github.com/hossein1387/BART-System-Verification"><img width="20px" src="./Files/logos/Github.png"></a>
          /
          <a href="https://www.youtube.com/watch?v=wO1HBslPy88"><img width="20px" src="./Files/logos/youtube.png"></a>

        <p align="justify" class="abstract">
          In this paper, we verified a train control system for safe speed and acceleration limits. We verified different properties of the model. For this task, we verified model properties using the NuSMV model checker ( a symbolic model checker tool). We then implement the algorithm of the verified model on an ARM CortexM platform.
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table> -->
      <!-- END OF POST -->


      <!-- POST -->
      <!-- <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Files/images/ICM2014.png" alt="prl" width="250" height="200"></td>
        <td width="75%" valign="top">
          <a href="https://ieeexplore.ieee.org/document/7071849">
            <papertitle>
              Automatic mapping of AF3 specifications to ARM cortex-M based FRDM platfrom
            </papertitle>
          </a>
          <br>
          <venuetitle> ICM 2014 </venuetitle>
          <br>
          <a href="https://ieeexplore.ieee.org/document/7071849"><img width="20px" src="./Files/logos/IEEE.png"></a> 

        <p align="justify" class="abstract">
          In this paper we proposed a tool to map verified AutoFOCUS3(AF3) models to executable C code for ARM Cortex-M family processors. To execute a model on a target platform, the model needs to be accurately translated, and the translation must be based on the hardware resources. In this respect, this work aims to close the gap between high level description and the implementation. As a case study, using the proposed methodology, we implemented a self stabilizing distributed clock synchronization protocol on a FRDM platform.
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table> -->
      <!-- END OF POST -->



      <!-- POST -->
      <!-- <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%"><img src="./Files/images/csndsp2012.png" alt="prl" width="250" height="200"></td>
        <td width="75%" valign="top">
          <a href="https://ieeexplore.ieee.org/document/6292771">
            <papertitle>
              Duplication Avoidance for Energy Efficient Wireless Sensor Networks
            </papertitle>
          </a>
          <br>
          <venuetitle> CSNDSP 2012 </venuetitle>
          <br>
            <a href="https://ieeexplore.ieee.org/document/6292771"><img width="20px" src="./Files/logos/IEEE.png"></a> 

        <p align="justify" class="abstract">
          Aduplication avoidance method is introduced for wireless sensor network that aims for high coverage area and low control overheads. To preventi energy holes creation the periodic selected transmitters are distributed uniformly. 
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table> -->
      <!-- END OF POST -->



      <br><br><br><br>
      <heading>Talks/Workshops</heading> <br> <br>

      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td width="25%">
          <img src="./Files/images/Logo_CMC_RGB_En.jpg" alt="prl" width="200" height="150">
        </td>
        <td></td>
        <td width="75%" valign="top">
          <a href="https://www.cmc.ca/workshop-accelerating-ai-montreal/">
            <papertitle>BARVINN: Barrel RISC-V Neural Network Accelerator</papertitle>
          </a>
          <br>
          <venuetitle> Accelerating AI 2021 – Challenges and Opportunities in Cloud and Edge Computing, May 4th </venuetitle>
          <br>
          <a href="https://www.cmc.ca/workshop-accelerating-ai-2021/"><img width="20px" src="./Files/logos/globe.png"></a> 
          /
          <a href="https://www.youtube.com/watch?v=233MqCATM9k&ab_channel=CMCMicrosystems"><img width="20px" src="./Files/logos/youtube.png"></a>

        <p align="justify" class="abstract">
          In this presentation, I talked about BARVINN, a Barrel RISC-V Neural Network Accelerator. 
        </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->


      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
          <td width="25%">
            <img src="./Files/images/Logo_CMC_RGB_En.jpg" alt="prl" width="200" height="150">
          </td>
          <td></td>


          <td width="75%" valign="top">
          <a href="https://www.cmc.ca/workshop-accelerating-ai-montreal/">
            <papertitle>Hardware Aware Acceleration For Deep Neural Network</papertitle>
          </a>
          <br>
          <venuetitle> CMC Workshop: Accelerating AI - Challenges and Opportunities in Cloud and Edge Computing, Mar 6th, 2020 </venuetitle>
          <br>
          <a href="https://www.cmc.ca/workshop-accelerating-ai-montreal/"><img width="20px" src="./Files/logos/globe.png"></a> 
          /
          <a href="./Files/docs/CMC_Presentaiton.pdf"><img width="20px" src="./Files/logos/presentation_ico.png"></a>

          <p align="justify" class="abstract">
            In this presentation, I talked about how to accelerate computation in Deep Neural Networks. Specifically, I talked about Quantization. Quantization in Deep Learning is a technique to reduce power, memory and computation time of deep neural networks. I talked about how one can improve the performance of a DNN using both software and hardware solutions.
          </p>
        <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->


      <!-- POST -->
      <table width="100%" align="center" border="0" cellpadding="20">
      <tbody><tr>
        <td></td>
        <td width="25%">
          <img src="./riscv_workshop/images/bahonar.png" alt="prl" width="130" height="150">
        </td>
        <td></td>


        <td width="75%" valign="top">
          <a href="http://hossein1387.github.io/riscv_workshop/index_en.html">
            <papertitle>Workshop on New Methods on Designing Digital Systems</papertitle>
          </a>
          <br>
          <venuetitle> CMC Workshop: Accelerating AI - Challenges and Opportunities in Cloud and Edge Computing, Mar 6th, 2020 </venuetitle>
          <br>
          <a href="https://www.cmc.ca/workshop-accelerating-ai-montreal/"><img width="20px" src="./Files/logos/globe.png"></a> 
          /
          <a href="./Files/docs/CMC_Presentaiton.pdf"><img width="20px" src="./Files/logos/presentation_ico.png"></a>

          <p align="justify" class="abstract">
            In this workshop, I reviewed the most popular open source tools for design and simulation of digital systems. The attendants got a chance to use these tools and developed a simple circuit to calculate GCD. In the second part of the workshop, I talked about RISC-V and Chisel. At the end of the workshop, the attendants got a chance to use chisel to designa and simulate a 3-stage pipelined RISC-V core.
          </p>
          <p></p>
        </td>
      </tr>
      </tbody></table>
      <!-- END OF POST -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          <a href="https://jonbarron.info/">Thanks Jon!</a>
        </font>
        </p>
        </td>
      </tr>
      </tbody></table>
    </td>
    </tr>
  </tbody></table>
  

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-74053389-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-74053389-1');
</script>

</body>

</html>